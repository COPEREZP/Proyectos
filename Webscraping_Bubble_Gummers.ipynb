{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "433cd87d-0755-41d0-af1e-928fb8be5bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El scraping se está ejecutando en segundo plano. Puedes hacer otras tareas.\n",
      "Procesando la página 1...\n",
      "Página 1 procesada, moviéndose a la siguiente...\n",
      "Procesando la página 2...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, TimeoutException, StaleElementReferenceException\n",
    "\n",
    "def scrape_products():\n",
    "    service = Service(r'C:\\Users\\constanza.perez\\OneDrive - Colgram\\Escritorio\\SELENIUM\\chromedriver.exe')\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    all_products_data = []\n",
    "    today_date = datetime.now().strftime('%d-%m-%Y')\n",
    "    sizes = ['20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40']\n",
    "\n",
    "    for page in range(1, 20):  # Ahora recorremos las páginas del 1 al 20\n",
    "        driver.get(f\"https://www.bubblegummers.cl/582-escolar?page={page}\")\n",
    "        print(f\"Procesando la página {page}...\")\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, 30).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'article.product-miniature')))\n",
    "        except TimeoutException:\n",
    "            print(f\"Timeout: no se encontraron productos en la página {page}.\")\n",
    "            continue\n",
    "\n",
    "        # Obtener los enlaces de los productos\n",
    "        product_links = driver.find_elements(By.CSS_SELECTOR, 'article.product-miniature a.thumbnail')\n",
    "\n",
    "        if not product_links:\n",
    "            print(f\"No se encontraron productos en la página {page}.\")\n",
    "            continue\n",
    "\n",
    "        for link in product_links:\n",
    "            try:\n",
    "                # Asegurarse de que cada enlace es único y navegar hacia él\n",
    "                product_url = link.get_attribute('href')\n",
    "                driver.execute_script(\"window.open(arguments[0]);\", product_url)  # Abrir el enlace en una nueva pestaña\n",
    "                driver.switch_to.window(driver.window_handles[-1])  # Cambiar a la nueva pestaña\n",
    "\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'h1')))\n",
    "\n",
    "                # Extraer los datos del producto\n",
    "                product_name = driver.find_element(By.CSS_SELECTOR, 'h1.h1[itemprop=\"name\"]').text.strip()\n",
    "                brand_name = 'Bubble Gummers'\n",
    "\n",
    "                # EXTRAER EL COLOR\n",
    "                try:\n",
    "                    # Buscar el li que contiene \"Color\" y extraer el texto\n",
    "                    color_element = driver.find_element(By.XPATH, \"//li[contains(text(),'Color:')]\")\n",
    "                    color_text = color_element.text.split('Color:')[1].strip()  # Obtener solo el color\n",
    "                except NoSuchElementException:\n",
    "                    color_text = ''  # Si no se encuentra, se deja vacío\n",
    "\n",
    "                item_id = None\n",
    "                script = 'return window.dataLayer;'\n",
    "                data_layer = driver.execute_script(script)\n",
    "\n",
    "                for item in data_layer:\n",
    "                    if 'ecommerce' in item and 'items' in item['ecommerce']:\n",
    "                        for product in item['ecommerce']['items']:\n",
    "                            if 'item_id' in product:\n",
    "                                item_id = product['item_id']\n",
    "                                break\n",
    "                    if item_id:\n",
    "                        break\n",
    "\n",
    "                sku_1 = item_id if item_id else 'N/A'\n",
    "\n",
    "                original_price = driver.find_element(By.CSS_SELECTOR, 'span.regular-price').text if driver.find_elements(By.CSS_SELECTOR, 'span.regular-price') else ''\n",
    "                final_price = driver.find_element(By.CSS_SELECTOR, 'span[itemprop=\"price\"]').text if driver.find_elements(By.CSS_SELECTOR, 'span[itemprop=\"price\"]') else ''\n",
    "\n",
    "                # Inicializa listas para tallas y stock\n",
    "                stock_dict = {size: 0 for size in sizes}\n",
    "\n",
    "                # Actualización del stock para cada talla usando el nuevo selector\n",
    "                for size in sizes:\n",
    "                    try:\n",
    "                        size_element = driver.find_element(By.CSS_SELECTOR, f'span[title=\"{size}\"]')  # Cambiar el selector\n",
    "                        stock_quantity = int(size_element.get_attribute('data-max'))  # Extraer el stock desde el atributo 'data-max'\n",
    "                        stock_dict[size] = stock_quantity\n",
    "                    except (NoSuchElementException, StaleElementReferenceException):\n",
    "                        stock_dict[size] = 0\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error al procesar talla {size}: {e}\")\n",
    "\n",
    "                image_element = driver.find_element(By.CSS_SELECTOR, 'img.js-qv-product-cover')\n",
    "                image_url = image_element.get_attribute('src') \n",
    "                image_url_final = image_url if image_url else 'N/A'\n",
    "\n",
    "                product_data = {\n",
    "                    'Nombre': product_name,\n",
    "                    'Marca': brand_name,\n",
    "                    'Color 1': color_text,  # Color extraído aquí\n",
    "                    'SKU 1': sku_1,\n",
    "                    'Precio Original': original_price,\n",
    "                    'Precio Final': final_price,\n",
    "                    'URL de la Imagen': image_url_final,\n",
    "                    'URL Página': driver.current_url,\n",
    "                    'Fecha': today_date,\n",
    "                }\n",
    "\n",
    "                product_data.update(stock_dict)\n",
    "                all_products_data.append(product_data)\n",
    "\n",
    "                # Guardar los datos en el archivo Excel\n",
    "                df = pd.DataFrame(all_products_data)\n",
    "                df.to_excel(r'C:\\Users\\constanza.perez\\OneDrive - Colgram\\Escritorio\\EN1.xlsx', index=False)\n",
    "\n",
    "                # Volver a la página principal (cerrar la pestaña actual)\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])  # Cambiar de vuelta a la pestaña principal\n",
    "\n",
    "                # Esperar un poco antes de procesar el siguiente producto\n",
    "                time.sleep(3)\n",
    "\n",
    "            except ElementClickInterceptedException:\n",
    "                print(f\"Error: Elemento bloqueado para hacer clic en el producto {link.get_attribute('href')}\")\n",
    "            except NoSuchElementException:\n",
    "                print(f\"Error al extraer datos del producto: {link.get_attribute('href')}\")\n",
    "            except TimeoutException:\n",
    "                print(f\"Timeout: al cargar la página del producto {link.get_attribute('href')}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error inesperado: {e}\")\n",
    "\n",
    "        print(f\"Página {page} procesada, moviéndose a la siguiente...\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "scraping_thread = threading.Thread(target=scrape_products)\n",
    "scraping_thread.start()\n",
    "\n",
    "print(\"El scraping se está ejecutando en segundo plano. Puedes hacer otras tareas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "619cb48d-043e-4a39-9b4b-166b7a02573c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El scraping se está ejecutando en segundo plano. Puedes hacer otras tareas.\n",
      "Procesando la página 1...\n",
      "Página 1 procesada, moviéndose a la siguiente...\n",
      "Procesando la página 2...\n",
      "Página 2 procesada, moviéndose a la siguiente...\n",
      "Procesando la página 3...\n",
      "Página 3 procesada, moviéndose a la siguiente...\n",
      "Procesando la página 4...\n",
      "Página 4 procesada, moviéndose a la siguiente...\n",
      "Procesando la página 5...\n",
      "Página 5 procesada, moviéndose a la siguiente...\n",
      "Procesando la página 6...\n",
      "Página 6 procesada, moviéndose a la siguiente...\n",
      "Procesando la página 7...\n",
      "Página 7 procesada, moviéndose a la siguiente...\n",
      "Procesando la página 8...\n",
      "Página 8 procesada, moviéndose a la siguiente...\n",
      "Procesando la página 9...\n",
      "Página 9 procesada, moviéndose a la siguiente...\n",
      "Procesando la página 10...\n",
      "Timeout: no se encontraron productos en la página 10.\n",
      "Procesando la página 11...\n",
      "Timeout: no se encontraron productos en la página 11.\n",
      "Procesando la página 12...\n",
      "Timeout: no se encontraron productos en la página 12.\n",
      "Procesando la página 13...\n",
      "Timeout: no se encontraron productos en la página 13.\n",
      "Procesando la página 14...\n",
      "Timeout: no se encontraron productos en la página 14.\n",
      "Procesando la página 15...\n",
      "Timeout: no se encontraron productos en la página 15.\n",
      "Procesando la página 16...\n",
      "Timeout: no se encontraron productos en la página 16.\n",
      "Procesando la página 17...\n",
      "Timeout: no se encontraron productos en la página 17.\n",
      "Procesando la página 18...\n",
      "Timeout: no se encontraron productos en la página 18.\n",
      "Procesando la página 19...\n",
      "Timeout: no se encontraron productos en la página 19.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, TimeoutException, StaleElementReferenceException\n",
    "\n",
    "def scrape_products():\n",
    "    service = Service(r'C:\\Users\\constanza.perez\\OneDrive - Colgram\\Escritorio\\SELENIUM\\chromedriver.exe')\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    all_products_data = []\n",
    "    today_date = datetime.now().strftime('%d-%m-%Y')\n",
    "\n",
    "    for page in range(1, 20):  # Ahora recorremos las páginas del 1 al 20\n",
    "        driver.get(f\"https://www.bubblegummers.cl/614-zapatos?page={page}\")\n",
    "        print(f\"Procesando la página {page}...\")\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, 30).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'article.product-miniature')))\n",
    "        except TimeoutException:\n",
    "            print(f\"Timeout: no se encontraron productos en la página {page}.\")\n",
    "            continue\n",
    "\n",
    "        # Obtener los enlaces de los productos\n",
    "        product_links = driver.find_elements(By.CSS_SELECTOR, 'article.product-miniature a.thumbnail')\n",
    "\n",
    "        if not product_links:\n",
    "            print(f\"No se encontraron productos en la página {page}.\")\n",
    "            continue\n",
    "\n",
    "        for link in product_links:\n",
    "            try:\n",
    "                # Asegurarse de que cada enlace es único y navegar hacia él\n",
    "                product_url = link.get_attribute('href')\n",
    "                driver.execute_script(\"window.open(arguments[0]);\", product_url)  # Abrir el enlace en una nueva pestaña\n",
    "                driver.switch_to.window(driver.window_handles[-1])  # Cambiar a la nueva pestaña\n",
    "\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'h1')))\n",
    "\n",
    "                # Extraer los datos del producto\n",
    "                product_name = driver.find_element(By.CSS_SELECTOR, 'h1.h1[itemprop=\"name\"]').text.strip()\n",
    "                brand_name = 'Bubble Gummers'\n",
    "\n",
    "                # EXTRAER EL COLOR\n",
    "                try:\n",
    "                    # Buscar el li que contiene \"Color\" y extraer el texto\n",
    "                    color_element = driver.find_element(By.XPATH, \"//li[contains(text(),'Color:')]\")\n",
    "                    color_text = color_element.text.split('Color:')[1].strip()  # Obtener solo el color\n",
    "                except NoSuchElementException:\n",
    "                    color_text = ''  # Si no se encuentra, se deja vacío\n",
    "\n",
    "                item_id = None\n",
    "                script = 'return window.dataLayer;'\n",
    "                data_layer = driver.execute_script(script)\n",
    "\n",
    "                for item in data_layer:\n",
    "                    if 'ecommerce' in item and 'items' in item['ecommerce']:\n",
    "                        for product in item['ecommerce']['items']:\n",
    "                            if 'item_id' in product:\n",
    "                                item_id = product['item_id']\n",
    "                                break\n",
    "                    if item_id:\n",
    "                        break\n",
    "\n",
    "                sku_1 = item_id if item_id else 'N/A'\n",
    "\n",
    "                original_price = driver.find_element(By.CSS_SELECTOR, 'span.regular-price').text if driver.find_elements(By.CSS_SELECTOR, 'span.regular-price') else ''\n",
    "                final_price = driver.find_element(By.CSS_SELECTOR, 'span[itemprop=\"price\"]').text if driver.find_elements(By.CSS_SELECTOR, 'span[itemprop=\"price\"]') else ''\n",
    "\n",
    "                # Inicializa el diccionario del stock, pero sin tallas\n",
    "                stock_dict = {}\n",
    "\n",
    "                # Obtener la imagen\n",
    "                image_element = driver.find_element(By.CSS_SELECTOR, 'img.js-qv-product-cover')\n",
    "                image_url = image_element.get_attribute('src') \n",
    "                image_url_final = image_url if image_url else 'N/A'\n",
    "\n",
    "                product_data = {\n",
    "                    'Nombre': product_name,\n",
    "                    'Marca': brand_name,\n",
    "                    'Color 1': color_text,  # Color extraído aquí\n",
    "                    'SKU 1': sku_1,\n",
    "                    'Precio Original': original_price,\n",
    "                    'Precio Final': final_price,\n",
    "                    'URL de la Imagen': image_url_final,\n",
    "                    'URL Página': driver.current_url,\n",
    "                    'Fecha': today_date,\n",
    "                }\n",
    "\n",
    "                product_data.update(stock_dict)\n",
    "                all_products_data.append(product_data)\n",
    "\n",
    "                # Guardar los datos en el archivo Excel\n",
    "                df = pd.DataFrame(all_products_data)\n",
    "                df.to_excel(r'C:\\Users\\constanza.perez\\OneDrive - Colgram\\Escritorio\\BURBLE.xlsx', index=False)\n",
    "\n",
    "                # Volver a la página principal (cerrar la pestaña actual)\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])  # Cambiar de vuelta a la pestaña principal\n",
    "\n",
    "                # Esperar un poco antes de procesar el siguiente producto\n",
    "                time.sleep(3)\n",
    "\n",
    "            except ElementClickInterceptedException:\n",
    "                print(f\"Error: Elemento bloqueado para hacer clic en el producto {link.get_attribute('href')}\")\n",
    "            except NoSuchElementException:\n",
    "                print(f\"Error al extraer datos del producto: {link.get_attribute('href')}\")\n",
    "            except TimeoutException:\n",
    "                print(f\"Timeout: al cargar la página del producto {link.get_attribute('href')}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error inesperado: {e}\")\n",
    "\n",
    "        print(f\"Página {page} procesada, moviéndose a la siguiente...\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "scraping_thread = threading.Thread(target=scrape_products)\n",
    "scraping_thread.start()\n",
    "\n",
    "print(\"El scraping se está ejecutando en segundo plano. Puedes hacer otras tareas.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
