{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f70ada00-cdd3-4c5d-8959-42a27b24937f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hay más productos. Terminando el scraping.\n",
      "Datos guardados exitosamente en C:\\Users\\constanza.perez\\OneDrive - Colgram\\Escritorio\\HHH1.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "import re  # Para usar expresiones regulares\n",
    "from datetime import datetime\n",
    "\n",
    "# Función de Scraping de Productos\n",
    "def scrape_products():\n",
    "    # Configuración del ChromeDriver\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Si no quieres ver el navegador en acción\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Deshabilitar GPU (puede ser útil en máquinas sin GPU)\n",
    "    chrome_options.add_argument(\"--no-sandbox\")  # Evitar errores en entornos Linux (si es necesario)\n",
    "    \n",
    "    # Crear el servicio con el webdriver_manager\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    \n",
    "    # Iniciar el navegador\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    # Página base para las URLs\n",
    "    base_url = \"https://www.hushpuppieskids.cl/nino/calzado?page=\"\n",
    "    \n",
    "    # Lista para almacenar los datos de los productos\n",
    "    all_products_data = []\n",
    "\n",
    "    # Empezamos con la página 1\n",
    "    page_number = 1\n",
    "\n",
    "    while True:\n",
    "        # Generar la URL de la página actual\n",
    "        url = base_url + str(page_number)\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Esperar que cargue la página\n",
    "\n",
    "        # Encontrar todos los productos en la página actual\n",
    "        products = driver.find_elements(By.CSS_SELECTOR, \"div.vtex-search-result-3-x-galleryItem\")\n",
    "        \n",
    "        # Si no hay productos, significa que no hay más páginas\n",
    "        if not products:\n",
    "            print(\"No hay más productos. Terminando el scraping.\")\n",
    "            break\n",
    "        \n",
    "        # Extraer los datos de los productos\n",
    "        for product in products:\n",
    "            try:\n",
    "                # Extraer el nombre del producto\n",
    "                product_name = product.find_element(By.CSS_SELECTOR, \".vtex-product-summary-2-x-productNameContainer\").text\n",
    "                \n",
    "                # Extraer el precio original (si existe)\n",
    "                original_price_parts = product.find_elements(By.CSS_SELECTOR, \".vtex-product-price-1-x-listPriceValue span\")\n",
    "                if original_price_parts:\n",
    "                    original_price = ''.join([part.text for part in original_price_parts])\n",
    "                else:\n",
    "                    original_price = None  # Si no existe un precio original\n",
    "                \n",
    "                # Extraer el precio final\n",
    "                final_price_parts = product.find_elements(By.CSS_SELECTOR, \".vtex-product-price-1-x-sellingPriceValue span\")\n",
    "                final_price = ''.join([part.text for part in final_price_parts if part.text.strip() != ''])\n",
    "\n",
    "                # Extraer el enlace de la imagen\n",
    "                image_url = product.find_element(By.CSS_SELECTOR, \"img\").get_attribute(\"src\")\n",
    "\n",
    "                # Extraer la URL del producto\n",
    "                product_link_element = product.find_element(By.CSS_SELECTOR, \"a\")\n",
    "                product_url = product_link_element.get_attribute(\"href\")\n",
    "                \n",
    "                # Obtener el SKU desde la URL de la imagen\n",
    "                sku_match = re.search(r'arquivos/ids/(\\d+)-', image_url)\n",
    "                if sku_match:\n",
    "                    sku = sku_match.group(1)  # Extraemos el SKU que está entre 'arquivos/ids/' y '-'\n",
    "                else:\n",
    "                    sku = None  # Si no se encuentra el SKU en la URL\n",
    "                \n",
    "                # Fecha de scraping (hoy)\n",
    "                date_scraped = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "                # Añadir los datos a la lista\n",
    "                all_products_data.append({\n",
    "                    \"Nombre\": product_name,\n",
    "                    \"Marca\": None,  # No tenemos la marca\n",
    "                    \"Color 1\": None,  # No tenemos el color\n",
    "                    \"SKU 1\": sku,\n",
    "                    \"Precio Original\": original_price,\n",
    "                    \"Precio Final\": final_price,\n",
    "                    \"URL de la Imagen\": image_url,\n",
    "                    \"URL Página\": product_url,\n",
    "                    \"Fecha\": date_scraped,\n",
    "                    \"0\": None, \"3\": None, \"6\": None, \"9\": None, \"0-6\": None, \"6-12\": None,\n",
    "                    \"talla-unica\": None, \"chica\": None, \"grande\": None, \"15\": None, \"16\": None,\n",
    "                    \"17\": None, \"18\": None, \"19\": None, \"3-6\": None, \"9-12\": None, \"18-24\": None,\n",
    "                    \"24-36\": None, \"12\": None, \"20\": None, \"21\": None, \"22\": None, \"23\": None,\n",
    "                    \"24\": None, \"25\": None, \"26\": None, \"27\": None, \"36\": None, \"0-1\": None,\n",
    "                    \"1-2\": None, \"1-3\": None, \"1-4\": None, \"2\": None, \"2-3\": None, \"3-4\": None,\n",
    "                    \"4\": None, \"4-6\": None, \"4-10\": None, \"6-8\": None, \"6-10\": None, \"8\": None,\n",
    "                    \"8-10\": None, \"10\": None, \"10-12\": None, \"14\": None, \"28\": None, \"29\": None,\n",
    "                    \"30\": None, \"31\": None, \"32\": None, \"150\": None, \"frutal\": None, \"citrico\": None,\n",
    "                    \"33\": None, \"34\": None, \"35\": None, \"37\": None, \"38\": None, \"39\": None, \"40\": None,\n",
    "                    \"41\": None, \"One\": None, \"GRUPO CATEGORIA\": None, \"CATEGORIA\": None, \"MUNDO\": None,\n",
    "                    \"Descuento\": None\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error al extraer datos de un producto: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Incrementar el número de página\n",
    "        page_number += 1\n",
    "\n",
    "    # Cerrar el navegador\n",
    "    driver.quit()\n",
    "\n",
    "    # Crear un DataFrame con los datos y ordenarlos por las columnas especificadas\n",
    "    column_order = [\n",
    "        \"Nombre\", \"Marca\", \"Color 1\", \"SKU 1\", \"Precio Original\", \"Precio Final\", \n",
    "        \"URL de la Imagen\", \"URL Página\", \"Fecha\", \"0\", \"3\", \"6\", \"9\", \"0-6\", \"6-12\", \n",
    "        \"talla-unica\", \"chica\", \"grande\", \"15\", \"16\", \"17\", \"18\", \"19\", \"3-6\", \"9-12\", \n",
    "        \"18-24\", \"24-36\", \"12\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"36\", \n",
    "        \"0-1\", \"1-2\", \"1-3\", \"1-4\", \"2\", \"2-3\", \"3-4\", \"4\", \"4-6\", \"4-10\", \"6-8\", \"6-10\", \n",
    "        \"8\", \"8-10\", \"10\", \"10-12\", \"14\", \"28\", \"29\", \"30\", \"31\", \"32\", \"150\", \"frutal\", \n",
    "        \"citrico\", \"33\", \"34\", \"35\", \"37\", \"38\", \"39\", \"40\", \"41\", \"One\", \"GRUPO CATEGORIA\", \n",
    "        \"CATEGORIA\", \"MUNDO\", \"Descuento\"\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(all_products_data)\n",
    "    df = df[column_order]  # Reordenar las columnas\n",
    "\n",
    "    # Guardar los datos en Excel usando el formato que prefieras\n",
    "    df.to_excel(r'C:\\Users\\constanza.perez\\OneDrive - Colgram\\Escritorio\\ZAP3.xlsx', index=False)\n",
    "\n",
    "    print(\"Datos guardados exitosamente en C:\\\\Users\\\\constanza.perez\\\\OneDrive - Colgram\\\\Escritorio\\\\HHH1.xlsx\")\n",
    "\n",
    "# Llamar a la función para hacer scraping\n",
    "scrape_products()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
